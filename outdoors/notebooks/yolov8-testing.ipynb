{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c0351c2-6479-4d08-a13d-c3f21428e0b3",
   "metadata": {},
   "source": [
    "# Testing detecting numbers with object detection with the Yolov8 model\n",
    "\n",
    "The documentation of Yolo with lots of examples can be found [here](https://docs.ultralytics.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8dfe8f3-ef20-4e52-b0e2-6df56211e312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ultralytics in /opt/conda/lib/python3.11/site-packages (8.2.75)\n",
      "Requirement already satisfied: albumentations in /opt/conda/lib/python3.11/site-packages (1.4.13)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.0 in /opt/conda/lib/python3.11/site-packages (from ultralytics) (1.26.4)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in /opt/conda/lib/python3.11/site-packages (from ultralytics) (3.9.1)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in /opt/conda/lib/python3.11/site-packages (from ultralytics) (4.10.0.84)\n",
      "Requirement already satisfied: pillow>=7.1.2 in /opt/conda/lib/python3.11/site-packages (from ultralytics) (10.4.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /opt/conda/lib/python3.11/site-packages (from ultralytics) (6.0.1)\n",
      "Requirement already satisfied: requests>=2.23.0 in /opt/conda/lib/python3.11/site-packages (from ultralytics) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /opt/conda/lib/python3.11/site-packages (from ultralytics) (1.14.0)\n",
      "Requirement already satisfied: torch>=1.8.0 in /opt/conda/lib/python3.11/site-packages (from ultralytics) (2.4.0)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /opt/conda/lib/python3.11/site-packages (from ultralytics) (0.19.0)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /opt/conda/lib/python3.11/site-packages (from ultralytics) (4.66.4)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from ultralytics) (6.0.0)\n",
      "Requirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.11/site-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in /opt/conda/lib/python3.11/site-packages (from ultralytics) (2.2.2)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /opt/conda/lib/python3.11/site-packages (from ultralytics) (0.13.2)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from ultralytics) (2.0.0)\n",
      "Requirement already satisfied: scikit-image>=0.21.0 in /opt/conda/lib/python3.11/site-packages (from albumentations) (0.24.0)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in /opt/conda/lib/python3.11/site-packages (from albumentations) (4.12.2)\n",
      "Requirement already satisfied: pydantic>=2.7.0 in /opt/conda/lib/python3.11/site-packages (from albumentations) (2.8.2)\n",
      "Requirement already satisfied: albucore>=0.0.13 in /opt/conda/lib/python3.11/site-packages (from albumentations) (0.0.13)\n",
      "Requirement already satisfied: eval-type-backport in /opt/conda/lib/python3.11/site-packages (from albumentations) (0.2.0)\n",
      "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /opt/conda/lib/python3.11/site-packages (from albumentations) (4.10.0.84)\n",
      "Requirement already satisfied: tomli>=2.0.1 in /opt/conda/lib/python3.11/site-packages (from albucore>=0.0.13->albumentations) (2.0.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (24.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from pydantic>=2.7.0->albumentations) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /opt/conda/lib/python3.11/site-packages (from pydantic>=2.7.0->albumentations) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.23.0->ultralytics) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.23.0->ultralytics) (2024.7.4)\n",
      "Requirement already satisfied: networkx>=2.8 in /opt/conda/lib/python3.11/site-packages (from scikit-image>=0.21.0->albumentations) (3.3)\n",
      "Requirement already satisfied: imageio>=2.33 in /opt/conda/lib/python3.11/site-packages (from scikit-image>=0.21.0->albumentations) (2.34.2)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /opt/conda/lib/python3.11/site-packages (from scikit-image>=0.21.0->albumentations) (2024.7.2)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /opt/conda/lib/python3.11/site-packages (from scikit-image>=0.21.0->albumentations) (0.4)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (3.15.4)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (1.13.0)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /opt/conda/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8.0->ultralytics) (12.6.20)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Storing environment yaml.\n",
      "Tracing installed libraries.\n"
     ]
    }
   ],
   "source": [
    "# Install the Python ultralytics library\n",
    "!pip install ultralytics albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ef58030-7256-4592-8a47-2ec819a9cb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and set variables\n",
    "import glob\n",
    "import json\n",
    "import shutil\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Literal\n",
    "\n",
    "\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import numpy as np\n",
    "from albumentations.core.composition import TransformsSeqType\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "data_dir = Path(\"/home/jovyan/work/outdoors/sample-data\")\n",
    "yolo_dir = data_dir / \"yolov8-tests\"\n",
    "annotations_dir = yolo_dir / \"annotations\"\n",
    "labels_dir = yolo_dir / \"labels\"\n",
    "training_dir = yolo_dir / \"training\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c276ca31-af94-43cc-a993-2a9d5e8c892a",
   "metadata": {},
   "source": [
    "## Creating images and labels from annotations\n",
    "\n",
    "The following code will create images and labels from the annotations made by Rik. These annotations can be found under `/home/jovyan/work/private/data/data_for_notebooks/yolov8-tests/annotations` (or to whatever `annotations_dir` is set). The images and labels will be written to `/home/jovyan/work/private/data/data_for_notebooks/yolov8-tests/labels` (or to whatever `labels_dir` is set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df43dd3b-c1f9-45d1-909d-ede4ffc6146f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/outdoors/sample-data/yolov8-tests/annotations/st-eustatius_1924_0002_DxO.jpg does not exist\n",
      "/home/jovyan/work/outdoors/sample-data/yolov8-tests/annotations/st-eustatius_1924_0004_DxO.jpg does not exist\n",
      "/home/jovyan/work/outdoors/sample-data/yolov8-tests/annotations/st-eustatius_1924_0006_DxO.jpg does not exist\n",
      "/home/jovyan/work/outdoors/sample-data/yolov8-tests/annotations/st-eustatius_1924_0008_DxO.jpg does not exist\n",
      "/home/jovyan/work/outdoors/sample-data/yolov8-tests/annotations/st-eustatius_1924_0010_DxO.jpg does not exist\n",
      "/home/jovyan/work/outdoors/sample-data/yolov8-tests/annotations/st-eustatius_1924_0012_DxO.jpg does not exist\n",
      "/home/jovyan/work/outdoors/sample-data/yolov8-tests/annotations/st-eustatius_1924_0014_DxO.jpg does not exist\n",
      "/home/jovyan/work/outdoors/sample-data/yolov8-tests/annotations/st-eustatius_1924_0016_DxO.jpg does not exist\n",
      "/home/jovyan/work/outdoors/sample-data/yolov8-tests/annotations/st-eustatius_1924_0018_DxO.jpg does not exist\n",
      "/home/jovyan/work/outdoors/sample-data/yolov8-tests/annotations/st-eustatius_1924_0020_DxO.jpg does not exist\n",
      "/home/jovyan/work/outdoors/sample-data/yolov8-tests/annotations/st-eustatius_1924_0022_DxO.jpg does not exist\n",
      "/home/jovyan/work/outdoors/sample-data/yolov8-tests/annotations/st-eustatius_1924_0024_DxO.jpg does not exist\n"
     ]
    }
   ],
   "source": [
    "def labels_from_annotations(annotations_dir, labels_dir):\n",
    "    annotation_json = annotations_dir / \"st-eustatius_002_Dx0.json\"\n",
    "\n",
    "    # Read annotation\n",
    "    with annotation_json.open(\"r\", encoding=\"UTF-8\") as fid:\n",
    "        annotations = json.load(fid)\n",
    "\n",
    "    for annotation in annotations.values():\n",
    "        filename = annotations_dir / annotation[\"filename\"]\n",
    "        if not filename.exists():\n",
    "            print(f\"{filename} does not exist\")\n",
    "            continue\n",
    "\n",
    "        # load image\n",
    "        image = cv2.imread(filename.as_posix())\n",
    "\n",
    "        for i, region in enumerate(annotation[\"regions\"]):\n",
    "            shape = region[\"shape_attributes\"]\n",
    "            attributes = region[\"region_attributes\"]\n",
    "            if shape[\"name\"] != \"rect\":\n",
    "                print(f\"Unsupported shape: {shape['name']}\")\n",
    "                continue\n",
    "            if str(attributes[\"char\"]) not in \"0123456789.\":\n",
    "                print(f\"Unsupported char: {attributes['char']}\")\n",
    "                continue\n",
    "            x, y, width, height = (\n",
    "                shape[\"x\"],\n",
    "                shape[\"y\"],\n",
    "                shape[\"width\"],\n",
    "                shape[\"height\"],\n",
    "            )\n",
    "            slice_i = slice(y, y + height)\n",
    "            slice_j = slice(x, x + width)\n",
    "            image_region = image[slice_i, slice_j, :]\n",
    "            # file_out\n",
    "            image_out = labels_dir / \"images\" / f\"{i}.png\"\n",
    "            image_out.parent.mkdir(parents=True, exist_ok=True)\n",
    "            label_out = labels_dir / \"labels\" / f\"{i}.txt\"\n",
    "            label_out.parent.mkdir(parents=True, exist_ok=True)\n",
    "            # Store it\n",
    "            cv2.imwrite(image_out.as_posix(), image_region)\n",
    "\n",
    "            # label = f'{attributes[\"char\"]} {x} {y} {width} {height}\\n'\n",
    "            with label_out.open(\"w\", encoding=\"UTF-8\") as fid:\n",
    "                char = attributes[\"char\"]\n",
    "                if char == \".\":\n",
    "                    char = \"10\"\n",
    "                fid.write(char)\n",
    "\n",
    "\n",
    "# Create the images and labels, errors about missing files can be ignored.\n",
    "labels_from_annotations(annotations_dir, labels_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e73782-68ec-4e6b-9db6-a03e92946eea",
   "metadata": {},
   "source": [
    "## Creating a Yolo compatible training set from the generated labels\n",
    "\n",
    "The following code will generate a Yolo compatible training set from the created labels. It will be split in `train`, `val` and `test`. The training set will be written to `/home/jovyan/work/private/data/data_for_notebooks/yolov8-tests/training` (or to whatever `training_dir` is set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51684816-9243-4ee7-ab09-ddf59eafa330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f88e4ea50244148b97a6141ae9eeb1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating YOLO labeled images for train split:   0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ebb554b0afa49c49f20c8b0b3f55009",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating YOLO labeled images for val split:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d49975109b5547acbac594212a4384d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating YOLO labeled images for test split:   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def generate_yolo_dataset(\n",
    "    old_ds_path: Path,\n",
    "    new_ds_path: Path,\n",
    "    train_ratio: float = 0.8,\n",
    "    val_ratio: float = 0.1,\n",
    "    n_images: int = 1000,\n",
    "):\n",
    "    \"\"\"Generates yolo dataset for all splits.\n",
    "\n",
    "    Single objects images must be placed in `old_ds_path/images` and stored in `.png` files.\n",
    "    Objects labels must be placed in `old_ds_path/labels` directory and stored in `.txt` files.\n",
    "\n",
    "    Args:\n",
    "        old_ds_path (Path): Directory path of dataset with source images and labels.\n",
    "        new_ds_path (Path): Directory path of new dataset.\n",
    "        train_ratio (float, optional): Ratio of source images used to\n",
    "            create train dataset. Defaults to 0.8.\n",
    "        val_ratio (float, optional): Ratio of source images used to\n",
    "            create val dataset. Defaults to 0.1.\n",
    "        n_images (int, optional): Total number of images to generate.\n",
    "            Split ratios are applied for that aswell\n",
    "\n",
    "    \"\"\"\n",
    "    test_ratio = 1 - train_ratio - val_ratio\n",
    "    all_images_filepaths = np.array(glob.glob(str(old_ds_path / \"images/*\")))\n",
    "    N = len(all_images_filepaths)\n",
    "    all_idxs = list(range(N))\n",
    "    n_train = int(N * train_ratio)\n",
    "    n_val = int(N * val_ratio)\n",
    "\n",
    "    train_idxs = random.choices(all_idxs, k=n_train)\n",
    "    all_idxs = list(set(all_idxs).difference(set(train_idxs)))\n",
    "\n",
    "    val_idxs = random.choices(all_idxs, k=n_val)\n",
    "    test_idxs = list(set(all_idxs).difference(set(val_idxs)))\n",
    "\n",
    "    train_image_filepaths = all_images_filepaths[train_idxs]\n",
    "    val_image_filepaths = all_images_filepaths[val_idxs]\n",
    "    test_image_filepaths = all_images_filepaths[test_idxs]\n",
    "\n",
    "    train_n_images = int(train_ratio * n_images)\n",
    "    val_n_images = int(val_ratio * n_images)\n",
    "    test_n_images = int(test_ratio * n_images)\n",
    "\n",
    "    pre_transform = A.Compose(\n",
    "        [\n",
    "            # A.RGBShift(r_shift_limit=128, g_shift_limit=128, b_shift_limit=128, p=0.5),\n",
    "            A.RGBShift(),\n",
    "            A.ChannelShuffle(p=0.3),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    obj_transform = A.Compose(\n",
    "        [\n",
    "            # A.InvertImg(p=0.3),\n",
    "            # A.RGBShift(r_shift_limit=128, g_shift_limit=128, b_shift_limit=128, p=0.5),\n",
    "            A.RGBShift(),\n",
    "            A.ChannelShuffle(p=0.3),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    post_transforms = [\n",
    "        A.Rotate(limit=10, border_mode=cv2.BORDER_REPLICATE, p=1),\n",
    "    ]\n",
    "\n",
    "    kwargs = dict(\n",
    "        new_ds_path=new_ds_path,\n",
    "        nrows_low=1,\n",
    "        nrows_high=8,\n",
    "        ncols_low=1,\n",
    "        ncols_high=8,\n",
    "        p_box=0.5,\n",
    "        mix_mode=\"and\",\n",
    "        imgsz=(640, 640),\n",
    "        pre_transform=pre_transform,\n",
    "        obj_transform=obj_transform,\n",
    "        post_transforms=post_transforms,\n",
    "    )\n",
    "\n",
    "    generate_yolo_split_data(\n",
    "        train_image_filepaths, split=\"train\", n_images=train_n_images, **kwargs\n",
    "    )\n",
    "    generate_yolo_split_data(\n",
    "        val_image_filepaths, split=\"val\", n_images=val_n_images, **kwargs\n",
    "    )\n",
    "    generate_yolo_split_data(\n",
    "        test_image_filepaths, split=\"test\", n_images=test_n_images, **kwargs\n",
    "    )\n",
    "\n",
    "\n",
    "def generate_yolo_split_data(\n",
    "    images_filepaths: np.ndarray,\n",
    "    new_ds_path: Path,\n",
    "    split: str,\n",
    "    n_images: int = 1,\n",
    "    nrows_low: int = 2,\n",
    "    nrows_high: int = 8,\n",
    "    ncols_low: int = 2,\n",
    "    ncols_high: int = 8,\n",
    "    p_box: float = 0.5,\n",
    "    mix_mode: Literal[\"and\", \"or\", \"equal\", \"random\"] = \"random\",\n",
    "    imgsz: tuple[int, int] = (640, 640),\n",
    "    pre_transform: A.Compose | None = None,\n",
    "    obj_transform: A.Compose | None = None,\n",
    "    post_transforms: TransformsSeqType | None = None,\n",
    "):\n",
    "    \"\"\"Generates yolo-like dataset using images from `images_filepaths`\n",
    "    and save this dataset to `new_ds_path`.\n",
    "    Objects labels must be placed in labels/ directory and stored in .txt files.\n",
    "\n",
    "    Grid size (nrows x ncols) is sampled for each example using randint(low, high).\n",
    "\n",
    "    Args:\n",
    "        images_filepaths (np.ndarray): Filepaths of single object images.\n",
    "        new_ds_path (Path): Directory path for the new dataset.\n",
    "        split (str): Split name.\n",
    "        n_images (int, optional): How much examples to generate. Defaults to 1.\n",
    "        nrows_low (int, optional): Low boundary of nrows for grid sampling. Defaults to 2.\n",
    "        nrows_high (int, optional): High boundary of nrows for grid sampling. Defaults to 8.\n",
    "        ncols_low (int, optional): Low boundary of ncols for grid sampling. Defaults to 2.\n",
    "        ncols_high (int, optional): High boundary of ncols for grid sampling. Defaults to 8.\n",
    "        p_box (float, optional): Probability that an object will be sampled\n",
    "            at [row, col] position. Defaults to 0.5.\n",
    "        mix_mode (Literal[\"and\", \"or\", \"equal\", \"random\"]): How to mix object box with background image.\n",
    "            \"and\" applies `&` operator, \"or\" applier `|` operator, \"equal\" sets object box directly,\n",
    "            \"random\" randomly choses one of [\"and\", \"or\", \"equal\"] for each box. Default to \"random\".\n",
    "        imgsz (tuple[int, int], optional): Desired image size ([height, width]).\n",
    "            Defaults to (640, 640).\n",
    "        pre_transform (A.Compose, optional): transform applied to the background of the grid\n",
    "            before sampling single objects.\n",
    "        obj_transform (A.Compose, optional): transform applied to each single object put on the grid.\n",
    "        post_transforms (TransformsSeqType, optional): transforms applied to the grid\n",
    "            after objects sampling.\n",
    "    \"\"\"\n",
    "    labels_filepaths = np.array(\n",
    "        [\n",
    "            path.replace(\"images/\", \"labels/\").replace(\".png\", \".txt\")\n",
    "            for path in images_filepaths\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    idxs = list(range(len(images_filepaths)))\n",
    "\n",
    "    dst_images_dirpath = new_ds_path / \"images\" / split\n",
    "    dst_labels_dirpath = new_ds_path / \"labels\" / split\n",
    "\n",
    "    dst_images_dirpath.mkdir(exist_ok=True, parents=True)\n",
    "    dst_labels_dirpath.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    for i in tqdm(\n",
    "        range(n_images), desc=f\"Generating YOLO labeled images for {split} split\"\n",
    "    ):\n",
    "        nrows = random.randint(nrows_low, nrows_high)\n",
    "        ncols = random.randint(ncols_low, ncols_high)\n",
    "        n_examples = nrows * ncols\n",
    "        random_idxs = random.choices(idxs, k=n_examples)\n",
    "        imgs = [\n",
    "            np.asarray(Image.open(filepath))\n",
    "            for filepath in images_filepaths[random_idxs]\n",
    "        ]\n",
    "        labels = [\n",
    "            read_text_file(filepath)[0] for filepath in labels_filepaths[random_idxs]\n",
    "        ]\n",
    "        transformed = generate_yolo_example(\n",
    "            imgs,\n",
    "            labels,\n",
    "            nrows=nrows,\n",
    "            ncols=ncols,\n",
    "            p_box=p_box,\n",
    "            mix_mode=mix_mode,\n",
    "            imgsz=imgsz,\n",
    "            pre_transform=pre_transform,\n",
    "            obj_transform=obj_transform,\n",
    "            post_transforms=post_transforms,\n",
    "        )\n",
    "        image = transformed[\"image\"]\n",
    "        bboxes = transformed[\"bboxes\"]\n",
    "        classes = transformed[\"labels\"]\n",
    "        if len(bboxes) == 0:\n",
    "            continue\n",
    "        Image.fromarray(image).save(str(dst_images_dirpath / f\"{i}.png\"))\n",
    "        txt_lines = []\n",
    "        for bbox, class_id in zip(bboxes, classes):\n",
    "            x_center, y_center, width, height = bbox\n",
    "            txt_lines.append(\n",
    "                \" \".join(\n",
    "                    [str(x) for x in [class_id, x_center, y_center, width, height]]\n",
    "                )\n",
    "            )\n",
    "        txt_annotation = \"\\n\".join(txt_lines)\n",
    "        save_txt_to_file(txt_annotation, dst_labels_dirpath / f\"{i}.txt\")\n",
    "\n",
    "\n",
    "def read_text_file(filename):\n",
    "    with open(filename, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "        lines = [line.strip() for line in lines]  # Optional: Remove leading/trailing whitespace\n",
    "\n",
    "    return lines\n",
    "\n",
    "\n",
    "def save_txt_to_file(txt, filename):\n",
    "    with open(filename, \"w\") as file:\n",
    "        file.write(txt)\n",
    "\n",
    "\n",
    "def generate_yolo_example(\n",
    "    imgs: list[np.ndarray],\n",
    "    class_ids: list[str],\n",
    "    nrows: int = 3,\n",
    "    ncols: int = 3,\n",
    "    p_box: float = 0.5,\n",
    "    mix_mode: Literal[\"and\", \"or\", \"equal\", \"random\"] = \"random\",\n",
    "    imgsz: tuple[int, int] = (640, 640),\n",
    "    pre_transform: A.Compose | None = None,\n",
    "    obj_transform: A.Compose | None = None,\n",
    "    post_transforms: TransformsSeqType | None = None,\n",
    ") -> dict[str, np.ndarray]:\n",
    "    \"\"\"Generate image for further yolo training.\n",
    "    First a grid `nrows x ncols` is created, then for each `i` cell in the grid\n",
    "    an `imgs[i]` image is sampled with `p_box` probability and randomly\n",
    "    placed in that cell.\n",
    "    This function is made for gray digits dataset (uses white background)\n",
    "    and applies `&` operator between a digit box and a background\n",
    "\n",
    "    Args:\n",
    "        imgs (list[np.ndarray]): List of single objects images\n",
    "        class_ids (list[str]): List of class ids for each object\n",
    "        nrows (int, optional): Number of grid rows. Defaults to 3.\n",
    "        ncols (int, optional): Number of grid cols. Defaults to 3.\n",
    "        p_box (float, optional): Probability that an object will be sampled\n",
    "            at [row, col] position. Defaults to 0.5.\n",
    "        mix_mode (Literal[\"and\", \"or\", \"equal\", \"random\"]): How to mix object box with background image.\n",
    "            \"and\" applies `&` operator, \"or\" applier `|` operator, \"equal\" sets object box directly,\n",
    "            \"random\" randomly choses one of [\"and\", \"or\", \"equal\"] for each box. Default to \"random\".\n",
    "        imgsz (tuple[int, int], optional): Desired image size ([height, width]).\n",
    "            Defaults to (640, 640).\n",
    "        pre_transform (A.Compose, optional): transform applied to the background of the grid\n",
    "            before sampling single objects.\n",
    "        obj_transform (A.Compose, optional): transform applied to each single object put on the grid.\n",
    "        post_transforms (TransformsSeqType, optional): transforms applied to the grid\n",
    "            after objects sampling.\n",
    "\n",
    "    Returns:\n",
    "        dict[str, np.ndarray]: Example YOLO training imput, that is a `dict` with\n",
    "            `image`, `bboxes`, `labels` keys.\n",
    "    \"\"\"\n",
    "\n",
    "    def mix_box_with_bg(\n",
    "        mode: Literal[\"and\", \"or\", \"equal\", \"random\"], bg_patch, box_img\n",
    "    ):\n",
    "        if mode == \"and\":\n",
    "            return bg_patch & box_img\n",
    "        elif mode == \"or\":\n",
    "            return bg_patch | box_img\n",
    "        elif mode == \"equal\":\n",
    "            return box_img\n",
    "        elif mode == \"random\":\n",
    "            return mix_box_with_bg(\n",
    "                random.choice([\"and\", \"or\", \"equal\"]), bg_patch, box_img\n",
    "            )\n",
    "\n",
    "    H, W = imgsz\n",
    "\n",
    "    box_h, box_w = H // nrows, W // ncols\n",
    "\n",
    "    margin_h = box_h // 2\n",
    "    margin_w = box_w // 2\n",
    "\n",
    "    BG_H, BG_W = H + margin_h * 2, W + margin_w * 2\n",
    "    bg_img = np.ones((BG_H, BG_W, 3)).astype(np.uint8) * 255\n",
    "    if pre_transform is not None:\n",
    "        bg_img = pre_transform(image=bg_img)[\"image\"]\n",
    "\n",
    "    bboxes = []\n",
    "    labels = []\n",
    "    # for i, (img, class_id) in enumerate(zip(imgs, class_ids)):\n",
    "    for i in range(nrows * ncols):  # make sure that only nrows * ncols images are used\n",
    "        if random.random() > p_box:\n",
    "            continue\n",
    "\n",
    "        img, class_id = imgs[i], class_ids[i]\n",
    "        row, col = i // ncols, i % ncols\n",
    "        box_x_min = col * box_w + margin_w\n",
    "        box_x_max = box_x_min + box_w\n",
    "        box_y_min = row * box_h + margin_h\n",
    "        box_y_max = box_y_min + box_h\n",
    "\n",
    "        h, w, *c = img.shape\n",
    "        w_ratio = box_w / w\n",
    "        h_ratio = box_h / h\n",
    "\n",
    "        if h_ratio < w_ratio:\n",
    "            fy = box_h / h\n",
    "            fx = fy\n",
    "        else:\n",
    "            fx = box_w / w\n",
    "            fy = fx\n",
    "        # img = cv2.resize(img, (0, 0), fx=fx, fy=fy)\n",
    "        # h, w, *c = img.shape\n",
    "        if len(c) == 0:  # gray -> RGB\n",
    "            # digits dataset is in gray, so repeating RGB to get 3 channels for YOLO\n",
    "            img = img[..., np.newaxis]\n",
    "            img = np.repeat(img, 3, axis=2)\n",
    "\n",
    "        x_center = random.randint(box_x_min + box_w // 3, box_x_max - box_w // 3)\n",
    "        y_center = random.randint(box_y_min + box_h // 3, box_y_max - box_h // 3)\n",
    "\n",
    "        left = w // 2\n",
    "        right = w - left\n",
    "\n",
    "        bottom = h // 2\n",
    "        top = h - bottom\n",
    "\n",
    "        x_min, x_max = x_center - left, x_center + right\n",
    "        y_min, y_max = y_center - bottom, y_center + top\n",
    "        if obj_transform is not None:\n",
    "            img = obj_transform(image=img)[\"image\"]\n",
    "        bg_patch = bg_img[y_min:y_max, x_min:x_max]\n",
    "        bg_img[y_min:y_max, x_min:x_max] = mix_box_with_bg(mix_mode, bg_patch, img)\n",
    "\n",
    "        x_center_n = x_center / BG_W\n",
    "        y_center_n = y_center / BG_H\n",
    "        w_n = w / BG_W\n",
    "        h_n = h / BG_H\n",
    "\n",
    "        bboxes.append([x_center_n, y_center_n, w_n, h_n])\n",
    "        labels.append(class_id)\n",
    "\n",
    "    bboxes = np.array(bboxes)\n",
    "    labels = np.array(labels)\n",
    "    transforms = [A.augmentations.crops.transforms.CenterCrop(H, W)]\n",
    "    if post_transforms is not None:\n",
    "        transforms.extend(post_transforms)\n",
    "    transform = A.Compose(\n",
    "        transforms,\n",
    "        bbox_params=A.BboxParams(format=\"yolo\", label_fields=[\"labels\"]),\n",
    "    )\n",
    "    transformed = transform(image=bg_img, bboxes=bboxes, labels=labels)\n",
    "    return transformed\n",
    "\n",
    "\n",
    "generate_yolo_dataset(labels_dir, training_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c4e022-3005-44c0-957e-a5a04e6d39bd",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Now you can train the model with Yolo. This can also be done in a Python script, but I used the command line. Of course you can change the `epochs` and `patience` (or add more parameters, see [the documentation](https://docs.ultralytics.com/modes/train/#train-settings) for more information). The console will show the directory where the run is logged (model, plots, etc.).\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "⚠ This doesn't seem to work (or is extremely slow) on the VRE. Locally (at least with an NVidia card) this works fine).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14466fd8-78be-4adf-ae52-c2b7c8d3a369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.2.76 available 😃 Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.2.75 🚀 Python-3.11.9 torch-2.4.0+cu121 CPU (Intel Xeon Platinum 8259CL 2.50GHz)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n, data=/home/jovyan/work/outdoors/sample-data/yolov8-tests/yolo_data_rescue_iw24.yaml, epochs=2000, time=None, patience=200, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train3, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train3\n",
      "Overriding model.yaml nc=80 with nc=11\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    753457  ultralytics.nn.modules.head.Detect           [11, [64, 128, 256]]          \n",
      "Model summary: 225 layers, 3,012,993 parameters, 3,012,977 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "/opt/conda/lib/python3.11/site-packages/ultralytics/engine/trainer.py:269: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(enabled=self.amp)\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/jovyan/work/private/data/data_for_notebooks/yolov8-tests/t\u001b[0m\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/jovyan/work/private/data/data_for_notebooks/yolov8-tests/tra\u001b[0m\n",
      "Plotting labels to runs/detect/train3/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train3\u001b[0m\n",
      "Starting training for 2000 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "!yolo \\\n",
    "    train \\\n",
    "    model=yolov8n \\\n",
    "    data=/home/jovyan/work/outdoors/sample-data/yolov8-tests/yolo_data_rescue_iw24.yaml \\\n",
    "    epochs=2000 \\\n",
    "    patience=200 \\\n",
    "    plots=true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7338db2e-96f2-44ac-adb7-a9701f219810",
   "metadata": {},
   "source": [
    "## Predictions\n",
    "\n",
    "Once you have a trained model, you can copy it from the `runs/trainX/weights` directory and give it a descriptive name. The file you should copy is called `best.pt`. As the name suggests this is from the epoch that produces the best result. 🙂\n",
    "\n",
    "You can now use this model to do predictions on an image.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "⚠ Before running this, make sure you have a model and an image and change the command accordingly!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2aa496c-5461-4445-ba5d-a9d8d207b8c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: your-copied-model: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!yolo predict model=<your-copied-model>.pt source=<image-to-predict-numbers> show=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59d49bc9-addb-4aa7-b4ee-c4aab344770d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ Environment does not support cv2.imshow() or PIL Image.show()\n",
      "The DISPLAY environment variable isn't set.\n",
      "Ultralytics YOLOv8.2.75 🚀 Python-3.11.9 torch-2.4.0+cu121 CPU (Intel Xeon Platinum 8259CL 2.50GHz)\n",
      "Model summary (fused): 168 layers, 11,129,841 parameters, 0 gradients, 28.5 GFLOPs\n",
      "\n",
      "image 1/1 /home/jovyan/work/outdoors/sample-data/yolov8-tests/annotations/st-eustatius_Feb_1910.jpg: 544x640 15 3s, 20 7s, 868.9ms\n",
      "Speed: 15.9ms preprocess, 868.9ms inference, 102.3ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict6\u001b[0m\n",
      "💡 Learn more at https://docs.ultralytics.com/modes/predict\n"
     ]
    }
   ],
   "source": [
    "path_to_trained_model = r\"/home/jovyan/work/outdoors/sample-data/yolov8-tests/models/datarescue_003.pt\"\n",
    "path_to_test_image_1 = r\"/home/jovyan/work/outdoors/sample-data/yolov8-tests/annotations/st-eustatius_Jan_1924.jpg\"\n",
    "path_to_test_image_2 = r\"/home/jovyan/work/outdoors/sample-data/yolov8-tests/annotations/st-eustatius_Jun_1911.jpg\"\n",
    "path_to_test_image_3 = r\"/home/jovyan/work/outdoors/sample-data/yolov8-tests/annotations/st-eustatius_Feb_1910.jpg\"\n",
    "\n",
    "!yolo predict model=/home/jovyan/work/outdoors/sample-data/yolov8-tests/models/datarescue_003.pt source=/home/jovyan/work/outdoors/sample-data/yolov8-tests/annotations/st-eustatius_Feb_1910.jpg show=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae24c4fc-e120-4ae2-b0f4-66ec20ad8dfc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
